
# ğŸ¤– LLM Whisper Advisor

A multilingual AI assistant web app built with **Vite + React + TypeScript** that provides compatibility suggestions for Large Language Models (LLMs) based on your system's hardware (especially RAM). It uses local models via **Ollama** or external APIs like **Google AI**, and supports multilingual interaction (English, Hindi, Telugu).

ğŸŒ [Live App](https://88fqf5q3-8080.inc1.devtunnels.ms/)

---

## âœ¨ Features

- ğŸ”€ **Model Switching** â€“ Choose from various LLMs (LLaMA3, Mistral, Gemma, etc.)
- ğŸ’¬ **Streamed AI Responses** â€“ Fast & real-time chatbot experience
- ğŸŒ **Multilingual Output** â€“ English, à¤¹à¤¿à¤‚à¤¦à¥€, à°¤à±†à°²à±à°—à±
- ğŸ™ï¸ **Voice Input** (Speech-to-Text) for user queries
- ğŸ“Š **RAM-Based LLM Suggestions** â€“ Suggests best LLMs based on available system memory
- ğŸ’¾ **Quantization & Model Details** â€“ Gives metadata like context length, model size, use-case
- ğŸ“¥ **Local or Cloud Inference** â€“ Use Ollama locally or deploy inference backend online
- ğŸ“ **Dynamic CSV/Excel Support** â€“ Uses structured model data for smart recommendations

---

## ğŸ› ï¸ Tech Stack

| Tool         | Description                       |
|--------------|-----------------------------------|
| Vite         | Fast build tool                   |
| React + TS   | Frontend UI with type safety      |
| Tailwind CSS | Styling                           |
| shadcn/ui    | UI components                     |
| i18next      | Internationalization support      |
| Ollama       | Local LLM inference engine        |
| Google AI    | Optional cloud-based LLM support  |

---

## ğŸ“¦ Installation & Setup

> **Requirements**: Node.js (18+), npm, Ollama (if using local models)

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/llm-whisper-advisor.git
cd llm-whisper-advisor-pilot
```

### 2. Install dependencies

```bash
npm install
```

### 3. Configure Environment

Create a `.env` file (example):

```env
VITE_OLLAMA_HOST=http://localhost:11434
VITE_SUPPORTED_LANGUAGES=en,hi,te
```

### 4. Start Development Server

```bash
npm run dev
```

The app will be available at [http://localhost:5173](http://localhost:5173).

---

## ğŸ§  How It Works

1. **User enters RAM capacity**
2. **App suggests compatible models** using structured data (`model_data.csv` or Excel)
3. **Assistant can chat** using selected model (via Ollama API or external backend)
4. **Supports translation and voice input** for multilingual users

---

## ğŸ“¤ Deployment (Vercel + Ollama Backend)

### Option 1: Local Use Only
Run Ollama and the frontend locally.

### Option 2: Public Use
Deploy frontend on Vercel and host Ollama on a cloud server (e.g., EC2) at a public IP.
- Update `.env` with public Ollama URL
- Make sure ports are open (`11434`)

---

## ğŸŒ Multilingual Support

- Built-in support for:
  - English (`en`)
  - Hindi (`hi`)
  - Telugu (`te`)
- Add more via `i18n.ts` config

---

## ğŸ“ Folder Structure

```
llm-whisper-advisor-pilot/
â”œâ”€â”€ public/               # Static assets
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/       # UI components
â”‚   â”œâ”€â”€ contexts/         # Language context
â”‚   â”œâ”€â”€ data/             # Model CSV or Excel data
â”‚   â”œâ”€â”€ pages/            # Main pages
â”‚   â””â”€â”€ utils/            # Helper functions
â”œâ”€â”€ package.json
â”œâ”€â”€ vite.config.ts
â””â”€â”€ tailwind.config.ts
```

---

## ğŸ¤– Models Supported

- ğŸ§  **LLaMA 2 / 3** (Meta)
- ğŸ§  **Mistral 7B / Mixtral**
- ğŸ§  **Gemma (Google)**
- ğŸ§  **Phi-2 / Phi-3**
- ğŸ§  **Zephyr, Dolphin, WizardLM**
- ğŸ§  **CodeLlama / DeepSeek-Coder**
- ...and more (70B quantized models too!)

---

## ğŸ§ª Development Scripts

```bash
npm run dev        # Run development server
npm run build      # Create production build
npm run preview    # Preview production build
npm run lint       # Lint code with ESLint
```

---

## ğŸ“– License

MIT Â© 2025 [Your Name or Org]

---

## ğŸ™‹â€â™‚ï¸ Contributions

Pull requests welcome! Please open an issue first for major changes.
